{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1e445f",
   "metadata": {},
   "source": [
    "### read in ATTOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# assessor data\n",
    "df_assessor = pd.read_csv('../../ATTOM/1_Assessor/assessor_master.csv')\n",
    "df_assessor = df_assessor[[\n",
    "    '[ATTOM ID]',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'year_built',\n",
    "    'area_building'\n",
    "]]\n",
    "\n",
    "# sales (recorder) data\n",
    "df_sales = pd.read_csv('../../ATTOM/2_Recorder/Recorder_master.csv')\n",
    "df_sales = df_sales.drop_duplicates(subset='TransactionID', keep='first')\n",
    "df_sales = df_sales[[\n",
    "    '[ATTOM ID]',\n",
    "    'TransferAmount',\n",
    "    'sale_year'\n",
    "]]\n",
    "\n",
    "# create new dataframe that merges assessor and sales data\n",
    "df_attom = pd.merge(df_assessor, df_sales, on='[ATTOM ID]', how='right')\n",
    "\n",
    "# filters\n",
    "df_attom = df_attom[df_attom['area_building'] <= 10000]\n",
    "df_attom['price_sf'] = df_attom['TransferAmount'] / df_attom['area_building']\n",
    "\n",
    "# drop any rows with null values in the lat/lon columns\n",
    "df_attom = df_attom.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "# convert to geopandas GeoDataFrame using the lat/lon columns\n",
    "df_attom_geo = gpd.GeoDataFrame(\n",
    "    df_attom, \n",
    "    geometry=gpd.points_from_xy(df_attom['lon'], df_attom['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061f3a1",
   "metadata": {},
   "source": [
    "### read in other geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in, clean up city data\n",
    "cities = gpd.read_file('cities.geojson')\n",
    "cities_to_keep = [\n",
    "    'Acworth',\n",
    "    'Atlanta',\n",
    "    'Austell',\n",
    "    'Avondale Estates',\n",
    "    'Ball Ground',\n",
    "    'Brookhaven',\n",
    "    'Canton',\n",
    "    'Chamblee',\n",
    "    'Clarkston',\n",
    "    'Conyers',\n",
    "    'Cumming',\n",
    "    'Dacula',\n",
    "    'Decatur',\n",
    "    'Doraville',\n",
    "    'Douglasville',\n",
    "    'Duluth',\n",
    "    'Dunwoody',\n",
    "    'East Point',\n",
    "    'Fayetteville',\n",
    "    'Forest Park',\n",
    "    'Grayson',\n",
    "    'Hampton',\n",
    "    'Hapeville',\n",
    "    'Johns Creek',\n",
    "    'Jonesboro',\n",
    "    'Kennesaw',\n",
    "    'Lawrenceville',\n",
    "    'Lithonia',\n",
    "    'Locust Grove',\n",
    "    'Marietta',\n",
    "    'McDonough',\n",
    "    'Milton',\n",
    "    'Morrow',\n",
    "    'Mountain Park',\n",
    "    'Norcross',\n",
    "    'Peachtree City',\n",
    "    'Powder Springs',\n",
    "    'Sandy Springs',\n",
    "    'Smyrna',\n",
    "    'Snellville',\n",
    "    'South Fulton',\n",
    "    'Stockbridge',\n",
    "    'Stonecrest',\n",
    "    'Sugar Hill',\n",
    "    'Suwanee',\n",
    "    'Union City',\n",
    "]\n",
    "cities = cities[['ShortLabel', 'geometry']]\n",
    "cities_atl = cities[cities['ShortLabel'].isin(cities_to_keep)].reset_index(drop=True)\n",
    "cities_atl = cities_atl.rename(columns={'ShortLabel': 'city'})\n",
    "\n",
    "# read in, clean up county data\n",
    "counties = gpd.read_file('../../Geographies/ARC_counties.gpkg')\n",
    "counties['NAME'] = counties['NAME'].str.replace(' County', '')\n",
    "counties = counties.rename(columns={'NAME': 'county'})\n",
    "\n",
    "# read in, clean up tract data\n",
    "tracts = gpd.read_file('../../Geographies/ARC_CTs.gpkg')\n",
    "tracts = tracts[['GEOID', 'geometry']]\n",
    "tracts = tracts.rename(columns={'GEOID': 'tract'})\n",
    "\n",
    "# read in submarket data, merge with tracts\n",
    "submarkets = pd.read_csv('HousingExplorer_data.csv')\n",
    "\n",
    "tracts_with_submarkets = tracts.merge(\n",
    "    submarkets, \n",
    "    left_on='tract', \n",
    "    right_on='Census Tract ID', \n",
    "    how='inner'\n",
    "    ).drop(columns=['Census Tract ID'])\n",
    "\n",
    "# set coordinate reference system\n",
    "tracts_with_submarkets = tracts_with_submarkets.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc931a",
   "metadata": {},
   "source": [
    "### spatial joins & clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set coordinate reference system of ATTOM to match other geographies\n",
    "df_attom_geo = df_attom_geo.to_crs(epsg=4326)\n",
    "\n",
    "# join ATTOM data to city, county, and tract data\n",
    "df_attom_geo_join1 = gpd.sjoin(df_attom_geo, cities_atl, how='left', predicate='within').drop(columns=['index_right'])\n",
    "df_attom_geo_join2 = gpd.sjoin(df_attom_geo_join1, counties, how='left', predicate='within').drop(columns=['index_right'])\n",
    "df_final = gpd.sjoin(df_attom_geo_join2, tracts_with_submarkets, how='left', predicate='within').drop(columns=['index_right'])\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_final = df_final.drop(columns='geometry')\n",
    "\n",
    "# drop all rows with null values in 'county' column\n",
    "df_final = df_final.dropna(subset=['county'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf59351",
   "metadata": {},
   "source": [
    "### advanced cleaning steps using duplicate methodology, modified z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c237ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying the clean_price_sf function to the df_clean dataframe\n"
     ]
    }
   ],
   "source": [
    "# create shallow copy\n",
    "df_clean = df_final.copy()\n",
    "\n",
    "# Group by both ATTOM_ID and year to count occurrences for each group\n",
    "grouped_counts_df = df_clean.groupby(\n",
    "    ['[ATTOM ID]', 'sale_year']).size().reset_index(name='sale_count')\n",
    "\n",
    "# Merge the grouped counts back into the original 'sales' DataFrame\n",
    "df_clean = df_clean.merge(grouped_counts_df, on=['[ATTOM ID]', 'sale_year'], how='left')\n",
    "\n",
    "\n",
    "# define function to compute the cleaned price / SF, which will calculate an \"aggregated\" price / sf if a property has sold more than once in a given year\n",
    "def clean_price_sf(row):\n",
    "    attom_id = row['[ATTOM ID]']\n",
    "    year = row['sale_year']\n",
    "    count = row['sale_count']\n",
    "\n",
    "    if count == 1:\n",
    "        return row['price_sf']\n",
    "    else:\n",
    "        subset = df_clean[(df_clean['[ATTOM ID]'] == attom_id)\n",
    "                       & (df_clean['sale_year'] == year)]\n",
    "        if count <= 3:\n",
    "            return subset['price_sf'].max()\n",
    "        else:\n",
    "            return subset['price_sf'].median()\n",
    "\n",
    "# just keep 2023 and 2024\n",
    "df_clean = df_clean[df_clean['sale_year'].isin([2019, 2024])]\n",
    "\n",
    "# Apply the de-dupe function defined above to create the 'price_sf_cleaned' column\n",
    "df_clean = df_clean.copy()\n",
    "\n",
    "print('applying the clean_price_sf function to the df_clean dataframe')\n",
    "df_clean['price_sf_cleaned'] = df_clean.apply(clean_price_sf, axis=1)\n",
    "\n",
    "# 2-step to drop the original 'price_sf' column and then rename the cleaned 'price_sf' back to its original name\n",
    "df_clean = df_clean.drop(columns='price_sf').rename(columns={\n",
    "    'price_sf_cleaned': 'price_sf'\n",
    "})\n",
    "\n",
    "\n",
    "# define the modified z-score outliers function\n",
    "def filter_by_modified_zscore(data, threshold=3.5):\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    if mad == 0:\n",
    "        return data  # No outliers if mad is 0\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    return data[np.abs(modified_z_scores) < threshold]\n",
    "\n",
    "\n",
    "# define the percentile outliers filter function\n",
    "# NOTE: this is not used in the final output, but is included for reference\n",
    "def filter_by_percentiles(data, lower_percentile, upper_percentile):\n",
    "    lower_bound = np.percentile(data, lower_percentile)\n",
    "    upper_bound = np.percentile(data, upper_percentile)\n",
    "    return data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "\n",
    "# define function to filter data by each method\n",
    "def filter_group(group, method, **kwargs):\n",
    "    prices = group['price_sf']\n",
    "    if method == 'modified_zscore':\n",
    "        filtered_prices = filter_by_modified_zscore(prices, **kwargs)\n",
    "    elif method == 'percentiles':\n",
    "        filtered_prices = filter_by_percentiles(prices, **kwargs)\n",
    "    return group[group['price_sf'].isin(filtered_prices)]\n",
    "\n",
    "\n",
    "sales_years = list(df_clean['sale_year'].unique())\n",
    "\n",
    "sales_list = []\n",
    "for year in sales_years:\n",
    "    df_year = df_clean[df_clean['sale_year'] == year]\n",
    "\n",
    "    filtered_sales_zscore = df_year.groupby('GEOID', group_keys=False).apply(\n",
    "        filter_group,\n",
    "        method='modified_zscore',\n",
    "        threshold=3.5,\n",
    "        include_groups=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    sales_list.append(filtered_sales_zscore)\n",
    "\n",
    "# create cleaned dataframe, inclusive of all years\n",
    "combined_cleaned_full_df = pd.concat(sales_list, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876382e",
   "metadata": {},
   "source": [
    "### perform aggregations by geo level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the Submarket column, replace 'Atlanta' with 'Atlanta-Sandy Springs-Marietta'\n",
    "combined_cleaned_full_df['Submarket'] = combined_cleaned_full_df['Submarket'].astype(str).str.replace('.0', '')\n",
    "combined_cleaned_full_df['Submarket_name'] = 'Submarket ' + combined_cleaned_full_df['Submarket'] \n",
    "\n",
    "# drop the Submarket column\n",
    "combined_cleaned_full_df = combined_cleaned_full_df.drop(columns=['Submarket'])\n",
    "\n",
    "# create 2019 and 2024 dataframes\n",
    "df_2019 = combined_cleaned_full_df[combined_cleaned_full_df['sale_year'] == 2019]\n",
    "df_2024 = combined_cleaned_full_df[combined_cleaned_full_df['sale_year'] == 2024]\n",
    "\n",
    "# 2019 data ------------------------------------------------------------\n",
    "# --- City-level aggregation ---\n",
    "city_df = (\n",
    "    df_2019[df_2019['city'].notna()]\n",
    "    .groupby('city')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'city': 'name'})\n",
    ")\n",
    "city_df['type'] = 'city'\n",
    "\n",
    "# --- County-level aggregation ---\n",
    "county_df = (\n",
    "    df_2019.groupby('county')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'county': 'name'})\n",
    ")\n",
    "county_df['type'] = 'county'\n",
    "\n",
    "# --- Submarket-level aggregation ---\n",
    "submarket_df = (\n",
    "    df_2019.groupby('Submarket_name')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'Submarket_name': 'name'})\n",
    ")\n",
    "submarket_df['type'] = 'submarket'\n",
    "\n",
    "# --- Regional-level aggregation ---\n",
    "region_data = {\n",
    "    'name': 'Atlanta Region',\n",
    "    'type': 'region',\n",
    "    'median_year_built': df_2019['year_built'].median(),\n",
    "    'median_area': df_2019['area_building'].median(),\n",
    "    'median_sale_price': df_2019['TransferAmount'].median(),\n",
    "    'median_price_sf': df_2019['price_sf'].median()\n",
    "}\n",
    "region_df = pd.DataFrame([region_data])\n",
    "\n",
    "# --- Combine all aggregations ---\n",
    "summary_df_2019 = pd.concat([city_df, county_df, submarket_df, region_df], ignore_index=True)\n",
    "\n",
    "# --- Final column ordering (optional but clean) ---\n",
    "summary_df_2019 = summary_df_2019[['name', 'type', 'median_year_built', 'median_area', 'median_sale_price', 'median_price_sf']]\n",
    "\n",
    "# export\n",
    "summary_df_2019.to_csv('ATTOM_2019.csv', index=False)\n",
    "\n",
    "# 2024 data ------------------------------------------------------------\n",
    "# --- City-level aggregation ---\n",
    "city_df = (\n",
    "    df_2024[df_2024['city'].notna()]\n",
    "    .groupby('city')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median'),\n",
    "        total_sales=('TransferAmount', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'city': 'name'})\n",
    ")\n",
    "city_df['type'] = 'city'\n",
    "\n",
    "# --- County-level aggregation ---\n",
    "county_df = (\n",
    "    df_2024.groupby('county')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median'),\n",
    "        total_sales=('TransferAmount', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'county': 'name'})\n",
    ")\n",
    "county_df['type'] = 'county'\n",
    "\n",
    "# --- Census tract-level aggregation ---\n",
    "tract_df = (\n",
    "    df_2024.groupby('tract')\n",
    "    .agg(\n",
    "        median_year_built=('year_built', 'median'),\n",
    "        median_area=('area_building', 'median'),\n",
    "        median_sale_price=('TransferAmount', 'median'),\n",
    "        median_price_sf=('price_sf', 'median'),\n",
    "        total_sales=('TransferAmount', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={'tract': 'name'})\n",
    ")\n",
    "tract_df['type'] = 'tract'\n",
    "\n",
    "# --- Regional-level aggregation ---\n",
    "region_data = {\n",
    "    'name': 'Atlanta Region',\n",
    "    'type': 'region',\n",
    "    'median_year_built': df_2024['year_built'].median(),\n",
    "    'median_area': df_2024['area_building'].median(),\n",
    "    'median_sale_price': df_2024['TransferAmount'].median(),\n",
    "    'median_price_sf': df_2024['price_sf'].median(),\n",
    "    'total_sales': df_2024['TransferAmount'].count()\n",
    "}\n",
    "region_df = pd.DataFrame([region_data])\n",
    "\n",
    "# --- Combine all aggregations ---\n",
    "summary_df_2024 = pd.concat([city_df, county_df, tract_df, region_df], ignore_index=True)\n",
    "\n",
    "# --- Final column ordering (optional but clean) ---\n",
    "summary_df_2024 = summary_df_2024[['name', 'type', 'total_sales', 'median_year_built', 'median_area', 'median_sale_price', 'median_price_sf']]\n",
    "\n",
    "# export\n",
    "summary_df_2024.to_csv('ATTOM_2024.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
